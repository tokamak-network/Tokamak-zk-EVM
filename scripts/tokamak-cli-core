#!/usr/bin/env bash

set -Eeuo pipefail
IFS=$'\n\t'

# Global config
VERBOSE=false
INSTALL_USE_BUN=false

# ---------- Importing functions ----------
SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
# shellcheck source=channel-functions.sh
source "$SCRIPT_DIR/channel-functions.sh"

# ---------- Pretty print ----------
log() { echo -e "\033[1;34m[tokamak-cli]\033[0m $*"; }
ok()  { echo -e "\033[1;32m[ ok ]\033[0m $*"; }
err() { echo -e "\033[1;31m[error]\033[0m $*" >&2; }
verbose() { if [[ "$VERBOSE" == true ]]; then echo -e "\033[1;36m[info]\033[0m $*" >&2; fi; }

# ---------- Helpers ----------
npm_install_safe() {
  local dir="$1"
  verbose "Installing npm dependencies in: $dir"
  pushd "$dir" >/dev/null
  if [[ "$VERBOSE" == true ]]; then
    npm install
  else
    npm install --silent
  fi
  popd >/dev/null
}

# Normalize Windows CRLF to LF before executing a shell script (best-effort)

to_unix_exec() {
  local f="$1"
  [[ -f "$f" ]] || return 0
  if command -v dos2unix >/dev/null 2>&1; then
    dos2unix -q "$f" || true
  fi
  chmod +x "$f" 2>/dev/null || true
}

# ---------- Minimal helpers for DRY ----------
ensure_file() { local f="$1"; [[ -f "$f" ]] || { err "Missing: $f"; return 1; }; }

abs_path_from_root() {
  local path="$1"
  [[ -n "$path" ]] || { echo ""; return 1; }
  if [[ "$path" != /* ]]; then
    echo "$ROOT/$path"
  else
    echo "$path"
  fi
}

# Run a script with timing; if second arg is 1, capture stdout to a temp log.
# Sets globals: RUN_START_MS, RUN_END_MS, RUN_OK(true/false), RUN_LOG(path or empty)
run_timed() {
  local script="$1"; local capture="${2:-0}"
  RUN_LOG=""; RUN_OK=true
  to_unix_exec "$script"
  RUN_START_MS="$(now_ms)"
  if [[ "$capture" == 1 ]]; then
    RUN_LOG="$(mktemp -t tokamak_stage.XXXXXX)"
    if bash "$script" | tee "$RUN_LOG"; then RUN_OK=true; else RUN_OK=false; fi
  else
    if bash "$script"; then RUN_OK=true; else RUN_OK=false; fi
  fi
  RUN_END_MS="$(now_ms)"
}

# Build pretty JSON array of checkpoints from a log file; prints to stdout
build_checkpoints_json_from_log() {
  local logfile="$1"
  [[ -s "$logfile" ]] || { echo "[]"; return; }
  if ! grep -F "Check point:" "$logfile" >/dev/null 2>&1; then echo "[]"; return; fi
  local lines
  lines=$(sed -n 's/^.*Check point: /Check point: /p' "$logfile")
  [[ -n "$lines" ]] || { echo "[]"; return; }
  local out="" first=true esc
  while IFS= read -r L; do
    esc="$(printf '%s' "$L" | json_escape)"
    if [[ "$first" == true ]]; then
      printf -v out '%s[\n          "%s"' "" "$esc"
      first=false
    else
      printf -v out '%s,\n          "%s"' "$out" "$esc"
    fi
  done <<< "$lines"
  printf -v out '%s\n        ]' "$out"
  echo "$out"
}

# ---------- Benchmark helpers ----------
now_ms() {
  # Prefer GNU date; fall back to python3/perl; last resort: seconds*1000
  local out=""
  if command -v gdate >/dev/null 2>&1; then
    out="$(gdate +%s%3N 2>/dev/null || true)"
    [[ "$out" =~ ^[0-9]{13,}$ ]] && { echo "$out"; return; }
  fi
  out="$(date +%s%3N 2>/dev/null || true)"
  [[ "$out" =~ ^[0-9]{13,}$ ]] && { echo "$out"; return; }
  if command -v python3 >/dev/null 2>&1; then
    python3 - <<'PY'
import time
print(int(time.time()*1000))
PY
    return
  fi
  if command -v perl >/dev/null 2>&1; then
    perl -MTime::HiRes=time -e 'printf("%d\n", int(time()*1000))'
    return
  fi
  echo $(( $(date +%s) * 1000 ))
}

iso_now() {
  # ISO8601 UTC; millisecond precision if gdate available
  if command -v gdate >/dev/null 2>&1; then
    gdate -u +"%Y-%m-%dT%H:%M:%S.%3NZ"
  else
    date -u +"%Y-%m-%dT%H:%M:%SZ"
  fi
}

rand_suffix() {
  # Generate a short [a-z0-9] suffix without fragile pipelines (pipefail/SIGPIPE-safe)
  if command -v python3 >/dev/null 2>&1; then
    python3 - <<'PY'
import secrets,string
alphabet='abcdefghijklmnopqrstuvwxyz0123456789'
print(''.join(secrets.choice(alphabet) for _ in range(9)))
PY
    return
  fi
  if command -v hexdump >/dev/null 2>&1; then
    # take 16 random bytes, hex-encode, then take first 9 chars
    hexdump -n 16 -v -e '/1 "%02x"' /dev/urandom 2>/dev/null | cut -c1-9
    return
  fi
  # last resort: timestamp-based fallback
  date +%s | cut -c1-9
}

json_escape() {
  # Escape a single line string for JSON (basic)
  sed -e 's/\\/\\\\/g' -e 's/\"/\\\"/g' -e 's/\t/\\t/g' -e 's/\r/\\r/g' -e 's/\n/\\n/g'
}

gather_hardware_json() {
  local platform release version arch cpu_model cores threads mem_total_gb mem_avail_gb gpu_name gpu_mem_mb cuda_supported
  arch="$(uname -m 2>/dev/null || echo unknown)"

  case "$(uname -s)" in
    Darwin)
      platform="macOS"
      if command -v sw_vers >/dev/null 2>&1; then
        release="$(sw_vers -productName) $(sw_vers -productVersion) $(sw_vers -buildVersion)"
      else
        release="macOS"
      fi
      version="darwin $(uname -r) ${arch}"
      cpu_model="$(sysctl -n machdep.cpu.brand_string 2>/dev/null || echo unknown)"
      cores="$(sysctl -n hw.physicalcpu 2>/dev/null || echo 0)"
      threads="$(sysctl -n hw.logicalcpu 2>/dev/null || echo 0)"
      # Memory
      local mem_bytes page_size free_pages
      mem_bytes="$(sysctl -n hw.memsize 2>/dev/null || echo 0)"
      page_size="$(sysctl -n hw.pagesize 2>/dev/null || echo 4096)"
      free_pages=$(vm_stat 2>/dev/null | awk '/free/ {gsub(".","",$3); print $3+0}' | head -n1)
      if [[ -n "$mem_bytes" && "$mem_bytes" != 0 ]]; then
        mem_total_gb=$(( mem_bytes / 1024 / 1024 / 1024 ))
      else
        mem_total_gb=0
      fi
      if [[ -n "$free_pages" && "$free_pages" != 0 ]]; then
        mem_avail_gb=$(( (free_pages * page_size) / 1024 / 1024 / 1024 ))
      else
        mem_avail_gb=0
      fi
      # GPU (fast path without heavy system_profiler)
      if command -v ioreg >/dev/null 2>&1; then
        gpu_name="$(ioreg -rc IOAccelerator 2>/dev/null | awk -F'= ' '/DeviceName|model/{gsub(/"/,"",$2); if($2!="") {print $2; exit}}')"
      fi
      gpu_name=${gpu_name:-unknown}
      gpu_mem_mb=0
      cuda_supported=false
      ;;
    Linux)
      platform="Linux"
      if [[ -r /etc/os-release ]]; then
        . /etc/os-release
        release="${PRETTY_NAME:-Linux}"
      else
        release="Linux"
      fi
      version="linux $(uname -r) ${arch}"
      cpu_model="$(awk -F: '/model name/ {gsub(/^ +/,"",$2); print $2; exit}' /proc/cpuinfo 2>/dev/null || echo unknown)"
      threads="$(getconf _NPROCESSORS_ONLN 2>/dev/null || nproc 2>/dev/null || echo 0)"
      cores="$threads"  # best-effort
      # Memory
      local mt ma
      mt="$(awk '/MemTotal:/ {print $2}' /proc/meminfo 2>/dev/null || echo 0)" # kB
      ma="$(awk '/MemAvailable:/ {print $2}' /proc/meminfo 2>/dev/null || echo 0)" # kB
      mem_total_gb=$(( mt / 1024 / 1024 ))
      mem_avail_gb=$(( ma / 1024 / 1024 ))
      # GPU
      if command -v nvidia-smi >/dev/null 2>&1; then
        gpu_name="$(nvidia-smi --query-gpu=name --format=csv,noheader 2>/dev/null | head -n1)"
        gpu_mem_mb="$(nvidia-smi --query-gpu=memory.total --format=csv,noheader,nounits 2>/dev/null | head -n1)"
        cuda_supported=true
      else
        gpu_name="$(lspci 2>/dev/null | grep -Ei 'vga|3d|2d' | head -n1 | sed 's/^.*: //')"
        gpu_mem_mb=0
        cuda_supported=false
      fi
      ;;
    *)
      platform="unknown"; release="unknown"; version="unknown"; cpu_model="unknown"; cores=0; threads=0; mem_total_gb=0; mem_avail_gb=0; gpu_name="unknown"; gpu_mem_mb=0; cuda_supported=false ;;
  esac

  cat <<JSON
{
  "cpu": {
    "model": "$(printf '%s' "$cpu_model" | json_escape)",
    "cores": ${cores:-0},
    "threads": ${threads:-0},
    "architecture": "$(printf '%s' "$arch" | json_escape)"
  },
  "memory": {
    "total": ${mem_total_gb:-0},
    "available": ${mem_avail_gb:-0}
  },
  "os": {
    "platform": "$(printf '%s' "$platform" | json_escape)",
    "release": "$(printf '%s' "$release" | json_escape)",
    "version": "$(printf '%s' "$version" | json_escape)"
  },
  "gpu": {
    "name": "$(printf '%s' "$gpu_name" | json_escape)",
    "memory": ${gpu_mem_mb:-0},
    "cudaSupported": $( [[ "$cuda_supported" == true ]] && echo true || echo false )
  }
}
JSON
}

make_zip() {
  local src_dir="$1" out_name="$2"
  ( cd "$src_dir" && rm -f "$out_name" && {
      if command -v zip >/dev/null 2>&1; then
        zip -r "$out_name" . >/dev/null
      elif command -v ditto >/dev/null 2>&1; then
        ditto -c -k --sequesterRsrc . "$out_name"
      else
        echo "zip tool not found (need 'zip' or macOS 'ditto')" >&2
        exit 1
      fi
    } )
}

# Discover repo root
resolve_root() {
  if [[ -n "${TOKAMAK_ZK_EVM_ROOT:-}" ]] && [[ -d "${TOKAMAK_ZK_EVM_ROOT}" ]]; then
    echo "${TOKAMAK_ZK_EVM_ROOT}"; return
  fi
  if command -v git >/dev/null 2>&1 && git rev-parse --is-inside-work-tree >/dev/null 2>&1; then
    git rev-parse --show-toplevel; return
  fi
  local script_dir="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
  if [[ -d "$script_dir/packages" ]]; then
    echo "$script_dir"; return
  fi
  err "Cannot locate repo root. Set TOKAMAK_ZK_EVM_ROOT, or run inside the repo."
  exit 1
}

ROOT="$(resolve_root)"
FE_QAP="$ROOT/packages/frontend/qap-compiler"
FE_SYN="$ROOT/packages/frontend/synthesizer"
BE_ROOT="$ROOT/packages/backend"

# OS target â†’ script/dir mapping
detect_target() {
  local uname_s; uname_s="$(uname -s)"
  if [[ "$uname_s" == "Darwin" ]]; then echo "mac"; return; fi
  if [[ -f /etc/os-release ]]; then
    . /etc/os-release
    case "${VERSION_ID:-}" in
      20*|20.*) echo "linux20"; return ;;
      22*|22.*) echo "linux22"; return ;;
    esac
  fi
  echo "linux22"
}

packaging_script_for_target() {
  # All platforms now use the unified packaging script
  echo "$ROOT/scripts/packaging.sh"
}

dist_dir_for_target() {
  echo "$ROOT/dist"
}

# Copy synthesizer outputs (after demo run) into backend dist resource
sync_synth_outputs_to_dist() {
  local src="$FE_SYN/outputs"
  local target dist_dir dest
  target="$(detect_target)"
  dist_dir="$(dist_dir_for_target "$target")"
  dest="$dist_dir/resource/synthesizer/output"

  log "Sync Synth outputs â†’ $dest (target=$target)"
  [[ -d "$src" ]] || { err "Synth outputs not found: $src"; exit 1; }
  mkdir -p "$dest"
  cp -af "$src/." "$dest/"
  ok "Synth outputs copied â†’ $dest"
  echo "$dest"
}

find_first_match_or_exit() {
  local src_dir="$1"
  local filename="$2"
  local label="${3:-$filename}"
  local matches first match_count
  matches="$(find "$src_dir" -type f -name "$filename" 2>/dev/null)"
  if [[ -z "$matches" ]]; then
    err "Missing $label under: $src_dir"
    exit 1
  fi
  first="$(printf '%s\n' "$matches" | head -n1)"
  match_count="$(printf '%s\n' "$matches" | wc -l | tr -d ' ')"
  if [[ "$match_count" -gt 1 ]]; then
    verbose "Multiple $label found; using: $first"
  fi
  printf '%s\n' "$first"
}

extract_zip_to_tmp_or_exit() {
  local zip_path_raw="$1"
  local tmp_prefix="$2"
  local zip_path
  zip_path="$(abs_path_from_root "$zip_path_raw")"
  [[ -f "$zip_path" ]] || { err "Zip file not found: $zip_path_raw"; exit 1; }
  if ! command -v unzip >/dev/null 2>&1; then
    err "unzip not found (required to extract: $zip_path_raw)"
    exit 1
  fi
  if ! unzip -t "$zip_path" >/dev/null 2>&1; then
    err "Invalid zip archive: $zip_path_raw"
    exit 1
  fi

  local tmp_dir
  tmp_dir="$(mktemp -d -t "${tmp_prefix}.XXXXXX")"

  if ! unzip -q "$zip_path" -d "$tmp_dir"; then
    rm -rf "$tmp_dir"
    err "Failed to extract zip: $zip_path_raw"
    exit 1
  fi

  printf '%s\n' "$tmp_dir"
}

sync_from_path() {
  local path_raw="${1:-}"
  local dir_handler="${2:-}"
  local zip_handler="${3:-}"
  [[ -n "$path_raw" ]] || { err "Path is required"; exit 1; }
  local path
  path="$(abs_path_from_root "$path_raw")"
  if [[ -d "$path" ]]; then
    "$dir_handler" "$path"
    return
  fi
  if [[ -f "$path" ]]; then
    "$zip_handler" "$path_raw"
    return
  fi
  err "Path not found: $path_raw"
  exit 1
}

sync_synth_outputs_from_dir() {
  local src_dir="${1:-}"
  [[ -n "$src_dir" ]] || { err "Source directory is required"; exit 1; }

  local required_files=(
    "instance_description.json"
    "instance.json"
    "permutation.json"
    "placementVariables.json"
  )

  local found_files=()
  local name
  for name in "${required_files[@]}"; do
    found_files+=("$(find_first_match_or_exit "$src_dir" "$name")")
  done

  local target dist_dir dest
  target="$(detect_target)"
  dist_dir="$(dist_dir_for_target "$target")"
  dest="$dist_dir/resource/synthesizer/output"

  log "Prove: syncing synthesizer outputs â†’ $dest"
  mkdir -p "$dest"
  local i
  for i in "${!required_files[@]}"; do
    cp -f "${found_files[$i]}" "$dest/${required_files[$i]}"
  done
  local state_matches state_first state_count
  state_matches="$(find "$src_dir" -type f -name "state_snapshot.json" 2>/dev/null)"
  if [[ -n "$state_matches" ]]; then
    state_first="$(printf '%s\n' "$state_matches" | head -n1)"
    state_count="$(printf '%s\n' "$state_matches" | wc -l | tr -d ' ')"
    if [[ "$state_count" -gt 1 ]]; then
      verbose "Multiple state_snapshot.json found; using: $state_first"
    fi
    cp -f "$state_first" "$dest/state_snapshot.json"
  fi

  ok "Synth outputs copied â†’ $dest"
}

sync_synth_outputs_from_zip() {
  local zip_path_raw="${1:-}"
  [[ -n "$zip_path_raw" ]] || { err "Zip path is required"; exit 1; }
  local tmp_dir
  tmp_dir="$(extract_zip_to_tmp_or_exit "$zip_path_raw" "tokamak_prove_zip")"
  sync_synth_outputs_from_dir "$tmp_dir"
  rm -rf "$tmp_dir"
}

sync_synth_outputs_from_path() {
  sync_from_path "$1" sync_synth_outputs_from_dir sync_synth_outputs_from_zip
}

sync_proof_from_dir() {
  local src_dir="${1:-}"
  [[ -n "$src_dir" ]] || { err "Source directory is required"; exit 1; }

  local target dist_dir
  target="$(detect_target)"
  dist_dir="$(dist_dir_for_target "$target")"

  log "Verify: syncing artifacts from $src_dir"

  # Sync required files
  local files_to_sync=(
    "resource/prove/output/proof.json"
    "resource/synthesizer/output/instance.json"
    "resource/preprocess/output/preprocess.json"
  )

  for rel_path in "${files_to_sync[@]}"; do
    local filename dest_path src_file
    filename="$(basename "$rel_path")"
    dest_path="$dist_dir/$rel_path"

    src_file="$(find_first_match_or_exit "$src_dir" "$filename")"

    mkdir -p "$(dirname "$dest_path")"
    cp -f "$src_file" "$dest_path"
    ok "Synced $filename â†’ $dest_path"
  done
}

sync_proof_from_zip() {
  local zip_path_raw="${1:-}"
  [[ -n "$zip_path_raw" ]] || { err "Zip path is required"; exit 1; }
  local tmp_dir
  tmp_dir="$(extract_zip_to_tmp_or_exit "$zip_path_raw" "tokamak_verify_zip")"
  sync_proof_from_dir "$tmp_dir"
  rm -rf "$tmp_dir"
}

sync_proof_from_path() {
  sync_from_path "$1" sync_proof_from_dir sync_proof_from_zip
}

run_synthesizer() {
  local config_path_raw="$1"
  [[ -n "$config_path_raw" ]] || { err "Synthesizer config path is required"; exit 1; }
  local config_path
  config_path="$(abs_path_from_root "$config_path_raw")"
  [[ -f "$config_path" ]] || { err "Synthesizer config not found: $config_path"; exit 1; }

  pushd "$FE_SYN" >/dev/null
  if [[ -f .env ]]; then
    verbose "Using RPC_URL from synthesizer/.env"
    npm run erc20-transfer-l2 "$config_path"
  else
    err "RPC_URL not configured. Run --install first or set RPC_URL environment variable"
    exit 1
  fi
  popd >/dev/null
}

step_install() {
  local install_arg="${1:-}"
  [[ -n "$install_arg" ]] || { err "--install requires <API_KEY|RPC_URL>"; exit 1; }

  log "Install: writing synthesizer .env"
  local rpc_url
  if [[ "$install_arg" == https://* ]]; then
    rpc_url="$install_arg"
  else
    rpc_url="https://eth-mainnet.g.alchemy.com/v2/${install_arg}"
  fi
  printf "RPC_URL='%s'\n" "$rpc_url" > "$FE_SYN/.env"

  log "Install: installing frontend dependencies"
  npm_install_safe "$FE_QAP"
  npm_install_safe "$FE_SYN"

  log "Install: compiling qap-compiler"
  pushd "$FE_QAP" >/dev/null
  [[ -f "./scripts/compile.sh" ]] || { err "Missing: $FE_QAP/scripts/compile.sh"; exit 1; }
  to_unix_exec "./scripts/compile.sh"
  bash "./scripts/compile.sh"
  popd >/dev/null

  local target script
  target="$(detect_target)"
  script="$(packaging_script_for_target "$target")"

  log "Install: running backend packaging for target=$target -> $(basename "$script")"
  # pushd "$BE_ROOT" >/dev/null
  [[ -f "$script" ]] || { err "Packaging script missing: $script"; exit 1; }
  to_unix_exec "$script"

  # Detect CI environment and set appropriate options
  # Different behavior based on CI job type:
  # - Proof Generation Test: needs fresh setup generation (--bun only)
  # - Binary Building: uses prebuilt setup (--bun --no-setup)

  local packaging_options=()

  # Add platform-specific option for unified script
  case "$target" in
    mac)     packaging_options+=("--macos") ;;
    linux20) packaging_options+=("--linux") ;;
    linux22) packaging_options+=("--linux") ;;
    *)       packaging_options+=("--linux") ;;
  esac
  packaging_options+=("--target-dir" "$ROOT/dist")

  if [[ "$INSTALL_USE_BUN" == true ]]; then
    packaging_options+=("--bun")
  fi

  if [ -z "${GITHUB_ACTIONS:-}" ] && [ -z "${CI:-}" ] && [ -z "${CONTINUOUS_INTEGRATION:-}" ]; then
    # Local environment: use default settings (npm) and skip compression
    packaging_options+=("--no-compress")
    log "Install: Local environment detected - using options: ${packaging_options[*]}"
  else
    # CI environment: use bun for performance, always skip compression
    # Note: setup generation depends on whether prebuilt files are available
    if [ -d "./prebuilt-setup" ] && [ "$(ls -A ./prebuilt-setup 2>/dev/null)" ]; then
      # Prebuilt setup available (Binary Building jobs)
      packaging_options+=("--bun" "--no-setup" "--no-compress")
      log "Install: CI environment with prebuilt setup - using options: ${packaging_options[*]}"
    else
      # No prebuilt setup (Proof Generation Test)
      packaging_options+=("--bun" "--no-compress")
      log "Install: CI environment, generating fresh setup - using options: ${packaging_options[*]}"
    fi
  fi

  bash "$script" "${packaging_options[@]}"
  # popd >/dev/null

  ok "Install complete"
}

step_synthesize() {
  local config_path_raw="${1:-}"
  [[ -n "$config_path_raw" ]] || { err "--synthesize requires <TX_CONFIG_JSON_PATH>"; exit 1; }

  local config_path
  config_path="$(abs_path_from_root "$config_path_raw")"
  [[ -f "$config_path" ]] || { err "Config JSON not found: $config_path_raw"; exit 1; }

  local target dist_dir proof_output synth_config_dest
  target="$(detect_target)"
  dist_dir="$(dist_dir_for_target "$target")"
  proof_output="$dist_dir/resource/prove/output"
  synth_config_dest="$dist_dir/resource/synthesizer/input/config.json"
  mkdir -p "$proof_output" "$(dirname "$synth_config_dest")"
  cp "$config_path" "$synth_config_dest"

  log "Synthesize: running frontend synthesizer with config=$config_path"
  run_synthesizer "$config_path"

  sync_synth_outputs_to_dist
  ok "Synthesize complete. Synth outputs in $dist_dir/resource/synthesizer/output"
  ok "Saved synth config â†’ $synth_config_dest"
}

step_preprocess() {
  local input_arg="${1:-}"
  local target dist_dir run3 backend_resource
  target="$(detect_target)"
  dist_dir="$(dist_dir_for_target "$target")"
  run3="$dist_dir/3_run-preprocess.sh"
  backend_resource="$dist_dir/resource/prove"

  if [[ -n "$input_arg" ]]; then
    local input_path synth_output_dir input_base dest_path
    input_path="$(abs_path_from_root "$input_arg")"
    if [[ ! -e "$input_path" ]]; then
      err "Preprocess input file not found: $input_arg"
      exit 1
    fi
    if [[ ! -f "$input_path" ]]; then
      err "Preprocess input must be a file (not a directory): $input_arg"
      exit 1
    fi
    input_base="$(basename "$input_path")"
    if [[ "$input_base" != "permutation.json" ]]; then
      err "Preprocess input must be permutation.json: $input_arg"
      exit 1
    fi
    synth_output_dir="$dist_dir/resource/synthesizer/output"
    mkdir -p "$synth_output_dir"
    dest_path="$synth_output_dir/permutation.json"
    cp "$input_path" "$dest_path"
    ok "Copied permutation.json â†’ $dest_path"
  fi

  log "Preprocess: running backend preprocess (target=$target)"
  ensure_file "$run3" || exit 1

  run_timed "$run3" 0
  local pre_s="$RUN_START_MS" pre_e="$RUN_END_MS" pre_ok=$RUN_OK
  if [[ "$pre_ok" != true ]]; then
    err "Preprocess failed"
    exit 1
  fi

  local pre_dur=$(( pre_e - pre_s ))
  ok "Preprocess complete (${pre_dur} ms) â†’ $backend_resource"
}

step_prove() {
  local input_arg="${1:-}"
  if [[ -n "$input_arg" ]]; then
    sync_synth_outputs_from_path "$input_arg"
  fi

  local target dist_dir run4 proof_output
  target="$(detect_target)"
  dist_dir="$(dist_dir_for_target "$target")"
  run4="$dist_dir/4_run-prove.sh"
  proof_output="$dist_dir/resource/prove/output"

  log "Prove: running backend prove (target=$target)"
  ensure_file "$run4" || exit 1
  mkdir -p "$proof_output"

  run_timed "$run4" 1
  local prove_s="$RUN_START_MS" prove_e="$RUN_END_MS" prove_ok=$RUN_OK
  local checkpoints_json="[]" pinit p0 p1 p2 p3 p4 ptotal
  if [[ -n "$RUN_LOG" && -s "$RUN_LOG" ]]; then
    checkpoints_json="$(build_checkpoints_json_from_log "$RUN_LOG")"
    pinit=$(sed -n 's/.*Prover init time: *\([0-9][0-9.]*\).*/\1/p' "$RUN_LOG" | tail -n1)
    p0=$(sed -n 's/.*prove0 running time: *\([0-9][0-9.]*\).*/\1/p' "$RUN_LOG" | tail -n1)
    p1=$(sed -n 's/.*prove1 running time: *\([0-9][0-9.]*\).*/\1/p' "$RUN_LOG" | tail -n1)
    p2=$(sed -n 's/.*prove2 running time: *\([0-9][0-9.]*\).*/\1/p' "$RUN_LOG" | tail -n1)
    p3=$(sed -n 's/.*prove3 running time: *\([0-9][0-9.]*\).*/\1/p' "$RUN_LOG" | tail -n1)
    p4=$(sed -n 's/.*prove4 running time: *\([0-9][0-9.]*\).*/\1/p' "$RUN_LOG" | tail -n1)
    ptotal=$(sed -n 's/.*Total proving time: *\([0-9][0-9.]*\).*/\1/p' "$RUN_LOG" | tail -n1)
  fi

  if [[ "$prove_ok" != true ]]; then
    err "Prove failed"
    exit 1
  fi

  local hw_json session_id ts_iso bench_json prove_dur total_dur
  session_id="benchmark_${prove_s}_$(rand_suffix)"
  ts_iso="$(iso_now)"
  hw_json="$(gather_hardware_json)"
  prove_dur=$(( prove_e - prove_s ))
  total_dur=$prove_dur
  bench_json="$proof_output/benchmark.json"

  cat > "$bench_json" <<JSON
{
  "sessionId": "${session_id}",
  "timestamp": "${ts_iso}",
  "hardwareInfo": ${hw_json},
  "processes": {
    "prove": {
      "startTime": ${prove_s},
      "endTime": ${prove_e},
      "duration": ${prove_dur},
      "success": true,
      "details": {
        "checkPoints": ${checkpoints_json},
        "proverInitTime": ${pinit:-null},
        "prove0Time": ${p0:-null},
        "prove1Time": ${p1:-null},
        "prove2Time": ${p2:-null},
        "prove3Time": ${p3:-null},
        "prove4Time": ${p4:-null},
        "totalProvingTime": ${ptotal:-null}
      }
    }
  },
  "metadata": {
    "cudaEnabled": $( command -v nvidia-smi >/dev/null 2>&1 && echo true || echo false ),
    "totalSessionDuration": ${total_dur}
  }
}
JSON

  ok "Benchmark written â†’ $bench_json"

  ok "Proof artifacts available in $proof_output"
}

step_extract_proof() {
  local out_path_raw="${1:-}"
  [[ -n "$out_path_raw" ]] || { err "--extract-proof requires <OUTPUT_ZIP_PATH>"; exit 1; }
  if [[ "$out_path_raw" == */ ]]; then
    err "--extract-proof requires a file path (not a directory): $out_path_raw"
    exit 1
  fi
  local out_path
  out_path="$(abs_path_from_root "$out_path_raw")"
  if [[ -d "$out_path" ]]; then
    err "--extract-proof requires a file path (not a directory): $out_path_raw"
    exit 1
  fi
  local out_dir
  out_dir="$(dirname "$out_path")"
  mkdir -p "$out_dir"

  local dist_dir tmp_dir zip_name
  dist_dir="$(dist_dir_for_target "$(detect_target)")"
  zip_name="$(basename "$out_path")"

  local preprocess_rel="resource/preprocess/output/preprocess.json"
  local preprocess_path="$dist_dir/$preprocess_rel"
  if [[ ! -f "$preprocess_path" ]]; then
    log "Extract proof: preprocess.json missing; running --preprocess first"
    step_preprocess
  fi

  tmp_dir="$(mktemp -d -t tokamak_extract.XXXXXX)"

  local artifacts=(
    "resource/synthesizer/output/instance.json"
    "resource/synthesizer/output/instance_description.json"
    "$preprocess_rel"
    "resource/prove/output/benchmark.json"
    "resource/prove/output/proof.json"
  )
  local optional_artifacts=(
    "resource/synthesizer/output/state_snapshot.json"
    "resource/synthesizer/output/previous_state_snapshot.json"
  )

  local missing=0
  for rel in "${artifacts[@]}"; do
    local src="$dist_dir/$rel"
    if [[ ! -f "$src" ]]; then
      err "Missing artifact: $src"
      missing=1
    fi
  done
  if [[ $missing -ne 0 ]]; then
    rm -rf "$tmp_dir"
    exit 1
  fi

  for rel in "${artifacts[@]}"; do
    local src="$dist_dir/$rel"
    local dest="$tmp_dir/$rel"
    mkdir -p "$(dirname "$dest")"
    cp "$src" "$dest"
  done
  for rel in "${optional_artifacts[@]}"; do
    local src="$dist_dir/$rel"
    if [[ -f "$src" ]]; then
      local dest="$tmp_dir/$rel"
      mkdir -p "$(dirname "$dest")"
      cp "$src" "$dest"
    fi
  done

  if make_zip "$tmp_dir" "$zip_name"; then
    mv -f "$tmp_dir/$zip_name" "$out_path"
    ok "Proof bundle written â†’ $out_path"
  else
    err "Failed to create proof archive"
    rm -rf "$tmp_dir"
    exit 1
  fi

  rm -rf "$tmp_dir"
}

step_verify() {
  local base_arg="${1:-}"

  local target dist_dir resource_dir proof_dir run5
  target="$(detect_target)"
  dist_dir="$(dist_dir_for_target "$target")"
  resource_dir="$dist_dir/resource"
  proof_dir="$resource_dir/prove/output"
  run5="$dist_dir/5_run-verify.sh"

  if [[ -n "$base_arg" ]]; then
    sync_proof_from_path "$base_arg"
  else
    [[ -d "$resource_dir" ]] || { err "resource directory not found: $resource_dir"; exit 1; }
  fi

  local preprocess_file="$resource_dir/preprocess/output/preprocess.json"
  local proof_file="$proof_dir/proof.json"
  local instance_file="$resource_dir/synthesizer/output/instance.json"
  local sigma_verify="$resource_dir/setup/output/sigma_verify.json"

  local missing=()
  [[ -f "$preprocess_file" ]] || missing+=("$preprocess_file (run --preprocess)")
  [[ -f "$proof_file" ]] || missing+=("$proof_file (run --prove)")
  [[ -f "$instance_file" ]] || missing+=("$instance_file (run --synthesize)")
  [[ -f "$sigma_verify" ]] || missing+=("$sigma_verify (run --preprocess to regenerate setup outputs)")

  if [[ ${#missing[@]} -gt 0 ]]; then
    err "Missing required artifacts for verification:"
    for m in "${missing[@]}"; do
      echo "  - $m" >&2
    done
    echo "ðŸ’¡ Run the indicated steps to generate the missing files, then re-run --verify." >&2
    exit 1
  fi

  [[ -f "$run5" ]] || { err "Missing verifier script: $run5"; exit 1; }

  log "Verify: using artifacts in $resource_dir"
  to_unix_exec "$run5"
  export ICICLE_BACKEND_INSTALL_DIR="${dist_dir}/backend-lib/icicle/lib/backend"
  local verify_out
  verify_out="$(bash "$run5" "$proof_dir" | tail -n1)"
  if [[ "$verify_out" == "true" || "$verify_out" == "true"$'\r' ]]; then
    ok "Verify: verify output => $verify_out"
  else
    err "Verify: verify output => $verify_out"
    exit 1
  fi

}

# System health check
step_doctor() {
  log "Running system health check..."

  local all_good=true

  # Check basic requirements
  for cmd in node npm circom rustc cmake dos2unix; do
    if command -v "$cmd" >/dev/null 2>&1; then
      if [[ "$VERBOSE" == true ]]; then
        case "$cmd" in
          node) verbose "node: $(node --version)" ;;
          npm) verbose "npm: $(npm --version)" ;;
          circom) verbose "circom: $(circom --version | head -1)" ;;
          rustc) verbose "rustc: $(rustc --version)" ;;
          *) verbose "$cmd: found" ;;
        esac
      fi
    else
      err "$cmd: not found"
      case "$cmd" in
        node) echo "ðŸ’¡ Install from https://nodejs.org/" >&2 ;;
        circom) echo "ðŸ’¡ Install from https://docs.circom.io/getting-started/installation/" >&2 ;;
        rustc) echo "ðŸ’¡ Install from https://www.rust-lang.org/tools/install" >&2 ;;
        cmake) echo "ðŸ’¡ Install from https://cmake.org/download/" >&2 ;;
        dos2unix) echo "ðŸ’¡ Install with: brew install dos2unix (macOS) or apt-get install dos2unix (Ubuntu)" >&2 ;;
      esac
      all_good=false
    fi
  done

  # Check project structure
  if [[ ! -d "$FE_QAP" ]]; then
    err "QAP compiler directory missing: $FE_QAP"
    all_good=false
  else
    verbose "QAP compiler directory: exists"
  fi

  if [[ ! -d "$FE_SYN" ]]; then
    err "Synthesizer directory missing: $FE_SYN"
    all_good=false
  else
    verbose "Synthesizer directory: exists"
  fi

  # Check if installed
  if [[ ! -f "$FE_SYN/.env" ]]; then
    err "Not installed - run --install first"
    echo "ðŸ’¡ Run: ./tokamak-cli --install <YOUR_API_KEY>" >&2
    all_good=false
  else
    verbose "Installation: complete (.env found)"
  fi

  # Check disk space (basic)
  local available_gb
  case "$(uname -s)" in
    Darwin)
      available_gb=$(df -g . | awk 'NR==2 {print $4}')
      ;;
    Linux)
      available_gb=$(df -BG . | awk 'NR==2 {print $4}' | tr -d 'G')
      ;;
    *)
      available_gb=0
      ;;
  esac

  if [[ $available_gb -lt 5 ]]; then
    err "Low disk space: ${available_gb}GB available (need 5GB+)"
    all_good=false
  else
    verbose "Disk space: ${available_gb}GB available"
  fi

  if [[ "$all_good" == true ]]; then
    ok "System is ready! ðŸš€"
  else
    err "Found issues that need attention"
    exit 1
  fi
}

# ---------- CLI entrypoint ----------
# shellcheck source=interface.sh
source "$SCRIPT_DIR/interface.sh"
